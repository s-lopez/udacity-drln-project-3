{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "This notebook describes the implementation and illustrates the live performance of Alice & Bob in the actual environment.\n",
    "\n",
    "# Background\n",
    "#### Multi-Agent Deep Deterministic Policy Gradients (MADDPG)\n",
    "Expanding on [DDPGs](https://arxiv.org/pdf/1509.02971.pdf), MADDPG is an algorithm that trains an ensemble of agents to solve a task. Agents can collaborate, compete, or a mixture of both. While one could try to implement DDPG separately for every agent, in [their paper](https://arxiv.org/pdf/1706.02275.pdf) the authors propose many improvements over this approach. Most importantly:\n",
    "* All agents share the same replay buffer\n",
    "* While actors can potentially only \"see\" a part of the environment state (their individual observation), the critics learn from the totality of the state and actions taken by all actors.\n",
    "\n",
    "\n",
    "# Parameters\n",
    "#### Actor\n",
    "The actor network has three layers, with sizes `24` (input), `512`, `256` and `2` (output). ReLUs are applied after the first and second layer. A tanh function is appliead after the last layer to ensure outputs in the range `[-1, 1]`. The actor applies noise to its actions as a Ornstein-Uhlenbeck process with $\\mu=0$, $\\theta=1$ and $\\sigma=0.15$. This noise decays after each episode by a factor of $0.95$\n",
    "\n",
    "#### Critic\n",
    "The actor network has three layers, with sizes `52` (input), `512`, `256` and `1` (output). ReLUs are applied after the first and second layer. The input size equals twice the state space size plus twice the action space size, since there are two agents.\n",
    "\n",
    "#### Learning\n",
    "This implementation uses the following learning parameters\n",
    "* A replay buffer of size `10⁶` is used\n",
    "* The mini-batch size is `400`\n",
    "* Discount rate $\\gamma=0.99$\n",
    "* Soft update rate $\\tau=10^{-3}$\n",
    "* Actor's learning rate $\\eta=10^{⁻4}$\n",
    "* Critic's learning rate $\\eta=5*10^{⁻4}$\n",
    "* Weight decay `= 0`\n",
    "\n",
    "# Results\n",
    "\n",
    "![Results](media/scores.png)\n",
    "\n",
    "The DDPG solves the environment after 4375 episodes.\n",
    "\n",
    "# Improvements\n",
    "\n",
    "The following are suggestions on how to improve the performance.\n",
    "* **Rewrite the replay buffer.** The replay buffer is implemented as a python deque, which performs fast queue/deque operations, but is [inefficient (O(n))](https://docs.python.org/3/library/collections.html#collections.deque) when sampling from the middle of the buffer. Having a faster replay buffer means faster trainings and more time for experiments.\n",
    "* **Perform a hyperparameter search.** There is guaranteed a better set of hyperparameters available, which would also speed up learning.\n",
    "* **Try a common critic.** Try training a shared critic instead of a separate critic for each agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent demonstration\n",
    "Below we will see the trained agents following a greedy policy in the actual environment.\n",
    "\n",
    "**Note:** Running the cell under \"Initialization\" will open a full-screen Unity window. Switch windows or run the whole script at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Invite our agent & import utils\n",
    "from ddpg_agent import Agent\n",
    "#from random import random as rnd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "PATH_TO_ENV = \"Tennis_Linux/Tennis.x86\"\n",
    "BRAIN = \"TennisBrain\"\n",
    "TRAINING = False\n",
    "\n",
    "env = UnityEnvironment(file_name=PATH_TO_ENV, no_graphics=TRAINING)\n",
    "\n",
    "ACTION_SIZE = env.brains[BRAIN].vector_action_space_size\n",
    "env_info = env.reset(train_mode=TRAINING)[BRAIN]\n",
    "NUM_AGENTS = len(env_info.agents)\n",
    "STATE_SIZE = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(env, actions, brain_name=BRAIN) -> tuple:\n",
    "    \"\"\"Sends actions to the environment env and observes the results.\n",
    "    Returns a tuple of rewards, next_states, dones (One per agent)\"\"\"\n",
    "    action_result = env.step(actions)[brain_name] # Act on the environment and observe the result\n",
    "    return (action_result.rewards,\n",
    "            action_result.vector_observations, # next states\n",
    "            action_result.local_done) # True if the episode ended\n",
    "    \n",
    "def reset(env, training=TRAINING, brain_name=BRAIN) -> np.ndarray:\n",
    "    \"\"\"Syntactic sugar for resetting the unity environment\"\"\"\n",
    "    return env.reset(train_mode=training)[brain_name].vector_observations\n",
    "\n",
    "def visualize(agents, env): \n",
    "    states = reset(env)\n",
    "    scores = np.zeros(NUM_AGENTS)\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = np.vstack([agent.decide(np.expand_dims(state, 0), as_tensor=False) \n",
    "                             for agent, state in zip(agents, states)]) # Choose actions\n",
    "        rewards, next_states, dones = act(env, actions)    # Send the action to the environment\n",
    "        scores += rewards[0]                                # Update the score\n",
    "        states = next_states                             # Roll over the state to next time step\n",
    "        done = np.any(dones)\n",
    "        time.sleep(.03)\n",
    "    print(\"Scores: {}\".format(scores))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agents\n",
    "agents = []\n",
    "state_dict = torch.load('models/20-01-13_21.18-4376-0.5.pth')\n",
    "for agent_name in ('Alice', 'Bob'):\n",
    "    agent = Agent(STATE_SIZE, ACTION_SIZE, NUM_AGENTS)\n",
    "    agent.actor_local.load_state_dict(state_dict[f'{agent_name}_actor_state_dict'])\n",
    "    agent.critic_local.load_state_dict(state_dict[f'{agent_name}_critic_state_dict'])\n",
    "    agents.append(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [1.19000002 1.19000002]\n"
     ]
    }
   ],
   "source": [
    "visualize(agents, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuts down the Unity environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
